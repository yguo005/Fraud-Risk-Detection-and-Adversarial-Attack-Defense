# -*- coding: utf-8 -*-
"""CS5100 final proj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18Sc759kyVsH2GvfdPjZjGCbBlAn0BTDf

#  Data Pre-processing
Dataset: [Bansim1](https://www.kaggle.com/datasets/ealaxi/banksim1/data)
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Read in the bank simulation data
data = pd.read_csv("bs140513_032310.csv")

# Preprocessing steps
# 1. Remove columns with constant values
data = data.drop(['zipcodeOri', 'zipMerchant'], axis=1)

# 2. Clean string columns by removing quotes
columns_to_clean = ['customer', 'age', 'gender', 'merchant', 'category']
for col in columns_to_clean:
    # Remove both single quotes and double quotes
    data[col] = data[col].str.strip("'").str.strip('"')

# 3. Remove 'es_' prefix from category column
data['category'] = data['category'].str.replace('es_', '', regex=False)

# 4. Remove rows where gender is Unknown ('U')
data = data[data['gender'] != 'U']

# 5. Replace 'U' with '7' in Age column
data.loc[data['age'] == 'U', 'age'] = '7'
data['age'] = pd.to_numeric(data['age']) # Convert 'age' to numeric

# 6. Create Amount Thresholds
def get_amount_threshold(amount):
    if amount <= 500:
        return '0-500'
    elif amount <= 1000:
        return '500-1000'
    elif amount <= 1500:
        return '1000-1500'
    elif amount <= 2000:
        return '1500-2000'
    elif amount <= 2500:
        return '2000-2500'
    elif amount <= 3000:
        return '2500-3000'
    else:
        return '>3000'

data['amount_thresh'] = data['amount'].apply(get_amount_threshold)

# remove rows where fraud is na
data = data.dropna(subset=['fraud'])

# Display first few rows of processed data
print("\nFirst few rows of processed data:")
print(data.head())

# Number of unique customers
unique_customers = data['customer'].nunique()
print(f"Total number of unique customers: {unique_customers:,}")

# Count unique merchants
unique_merchants = data['merchant'].nunique()
print(f"Total number of unique merchants: {unique_merchants:,}")

# Count the occurrences of fraud and no fraud
fraud_counts = data['fraud'].value_counts()

print("\nFraud and No Fraud Counts:")
print(fraud_counts)

"""#  Exploratory Data Analysis (EDA)"""

# Calculate average spending by gender
gender_spending = data.groupby('gender')['amount'].mean().reset_index()
gender_spending['Metric'] = 'gender'
gender_spending = gender_spending.rename(columns={'gender': 'Value', 'amount': 'avg_spent'})

# Calculate average spending by age
age_spending = data.groupby('age')['amount'].mean().reset_index()
age_spending['Metric'] = 'age'
age_spending = age_spending.rename(columns={'age': 'Value', 'amount': 'avg_spent'})

# Calculate average spending by category
category_spending = data.groupby('category')['amount'].mean().reset_index()
category_spending['Metric'] = 'category'
category_spending = category_spending.rename(columns={'category': 'Value', 'amount': 'avg_spent'})

# Combine all dataframes
combined_data = pd.concat([gender_spending, age_spending, category_spending])

# Create the visualization
plt.figure(figsize=(18, 7))

# Create subplots for each metric
metrics = ['category', 'age', 'gender']
colors = ['#FF9E9E', '#97C1A9', '#98B4D4']  # Example colors, adjust as needed

for idx, metric in enumerate(metrics, 1):
    plt.subplot(1, 3, idx)

    # Filter data for current metric
    metric_data = combined_data[combined_data['Metric'] == metric].copy()

    # Sort by average spent
    metric_data = metric_data.sort_values('avg_spent')

    # Create horizontal bar plot
    bars = plt.barh(y=range(len(metric_data)),
                   width=metric_data['avg_spent'],
                   color=colors[idx-1])

    # Add value labels
    for i, bar in enumerate(bars):
        plt.text(bar.get_width(),
                bar.get_y() + bar.get_height()/2,
                f'${metric_data.iloc[i]["avg_spent"]:,.0f}',
                va='center')

    # Customize plot
    plt.title(metric.capitalize())
    plt.xlabel('Average Spent ($)')
    if idx == 1:
        plt.ylabel('Value')

    # Set y-axis labels
    plt.yticks(range(len(metric_data)), metric_data['Value'])

# Adjust layout
plt.suptitle('Biggest Spenders', fontsize=16)

plt.tight_layout()

# Show plot
plt.show()

# Print the numerical results
print("\nAverage Spending by Gender:")
print(gender_spending)
print("\nAverage Spending by Age:")
print(age_spending)
print("\nAverage Spending by Category:")
print(category_spending)

# Set figure size
plt.figure(figsize=(18, 7))

# Define the order of amount thresholds
threshold_order = ['0-500', '500-1000', '1000-1500', '1500-2000',
                  '2000-2500', '2500-3000', '>3000']

# Calculate percentage of fraud for each amount threshold
fraud_percentages = (data.groupby('amount_thresh')['fraud']
                    .value_counts(normalize=True)
                    .mul(100)
                    .unstack()
                    .reindex(threshold_order))

# Rename columns for clarity
fraud_percentages.columns = ['Non-Fraud', 'Fraud']

# Create horizontal stacked bar plot
ax = fraud_percentages.plot(kind='barh',
                          stacked=True,
                          color=['#2ecc71', '#e74c3c'],  # Green for non-fraud, Red for fraud
                          width=0.8)

# Customize the plot
plt.title('Fraud Distribution Across Amount Thresholds',
         fontsize=16,
         pad=20)
plt.suptitle('Fraud probability increases with transaction amount',
            y=0.95,
            fontsize=12,
            style='italic')

# Format axes
plt.xlabel('Percentage (%)', fontsize=12)
plt.ylabel('Amount Threshold', fontsize=12)

# Add percentage labels on bars
for c in ax.containers:
    # Add percentage labels in the middle of each segment
    labels = [f'{v:.1f}%' if v >= 2 else '' for v in c.datavalues]
    ax.bar_label(c, labels=labels, label_type='center', fontsize=10, color='white')

# Customize legend
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

# Add grid
plt.grid(axis='x', linestyle='--', alpha=0.3)

# Set background color
ax.set_facecolor('#f8f9fa')
plt.gca().set_axisbelow(True)

# Adjust layout
plt.tight_layout()

# Show plot
plt.show()

# Print the actual percentages
print("\nFraud Distribution by Amount Threshold:")
print(fraud_percentages.round(2))

# Set figure size
plt.figure(figsize=(18, 7))

# Calculate counts by category (excluding transportation)
# About 500,000 transactions made alone in the transportation category, while the rest 90,000 transactions split unevenly between the other 14 categories left.
category_counts = (data[data['category'] != 'transportation']
                  .groupby('category')
                  .size()
                  .reset_index(name='count')
                  .sort_values('count', ascending=True))  # Sort for horizontal bars

# Create color gradient
counts = category_counts['count']
colors = plt.cm.RdYlBu_r(np.linspace(0.2, 0.8, len(counts)))  # Color gradient

# Create horizontal bar plot
bars = plt.barh(y=range(len(category_counts)),
                width=category_counts['count'],
                color=colors)

# Add value labels on bars
for i, bar in enumerate(bars):
    count = int(category_counts.iloc[i]['count'])
    formatted_count = "{:,}".format(count)  # Format number with commas
    plt.text(bar.get_width() + (max(counts) * 0.01),  # Add small padding
            bar.get_y() + bar.get_height()/2,
            formatted_count,
            va='center',
            ha='left',
            fontsize=10,
            bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))

# Customize plot
plt.title('Category Transaction Frequency', pad=20, fontsize=14)


# Set axis labels
plt.xlabel('Frequency')
plt.ylabel('Category')

# Set y-axis ticks
plt.yticks(range(len(category_counts)),
           category_counts['category'])

# Remove x-axis ticks
plt.xticks([])

# Add grid
plt.grid(axis='x', linestyle='--', alpha=0.3)

# Adjust layout
plt.tight_layout()

# Add some padding to the right for labels
plt.margins(x=0.1)

# Show plot
plt.show()

# Print category counts
print("\nTransaction Counts by Category:")
print(category_counts.sort_values('count', ascending=False))

# Set figure size
plt.figure(figsize=(18, 7))

# Calculate fraud counts by category
fraud_by_category = (data[data['fraud'] == 1]
                    .groupby('category')
                    .size()
                    .reset_index(name='count')
                    .sort_values('count', ascending=True))  # Sort for horizontal bars

# Create color gradient
counts = fraud_by_category['count']
colors = plt.cm.RdYlBu_r(np.linspace(0.2, 0.8, len(counts)))  # Color gradient from blue to red

# Create horizontal bar plot
bars = plt.barh(y=range(len(fraud_by_category)),
                width=fraud_by_category['count'],
                color=colors)

# Add value labels on bars
for i, bar in enumerate(bars):
    count = int(fraud_by_category.iloc[i]['count'])
    formatted_count = "{:,}".format(count)  # Format number with commas
    plt.text(bar.get_width() + (max(counts) * 0.01),  # Add small padding
            bar.get_y() + bar.get_height()/2,
            formatted_count,
            va='center',
            ha='left',
            fontsize=10,
            bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))

# Customize plot
plt.title('Fraud in Category Transaction', pad=20, fontsize=14)


# Set axis labels
plt.xlabel('Frequency')
plt.ylabel('Category')

# Set y-axis ticks
plt.yticks(range(len(fraud_by_category)),
           fraud_by_category['category'])

# Remove x-axis ticks
plt.xticks([])

# Add grid
plt.grid(axis='x', linestyle='--', alpha=0.3)

# Adjust layout to prevent label cutoff
plt.tight_layout()

# Add some padding to the right for labels
plt.margins(x=0.1)

# Show plot
plt.show()

# Print fraud counts by category
print("\nFraud Counts by Category:")
print(fraud_by_category.sort_values('count', ascending=False))

# Set figure size
plt.figure(figsize=(18, 7))

# Before creating the visualization, create a mapping of encoded values to original categories
category_mapping = dict(zip(range(len(le.classes_)), le.classes_))

# Map the encoded categories back to their original names
data['category_name'] = data['category'].map(category_mapping)

# Calculate fraud percentages using category_name instead of encoded category
fraud_by_category = (data.groupby('category_name')['fraud']
                    .value_counts(normalize=True)
                    .mul(100)
                    .unstack()
                    .fillna(0))

# Sort categories by fraud percentage
fraud_by_category = fraud_by_category.sort_values(by=1, ascending=True)  # Sort by fraud percentage

# Create horizontal stacked bar plot
ax = fraud_by_category.plot(kind='barh',
                          stacked=True,
                          color=['#2ecc71', '#e74c3c'],  # Green for non-fraud, Red for fraud
                          width=0.8)

# Customize the plot
plt.title('Fraud Distribution Across Categories',
         fontsize=16,
         pad=20)

# Format axes
plt.xlabel('Percentage (%)', fontsize=12)
plt.ylabel('Category', fontsize=12)

# Add percentage labels on bars
for c in ax.containers:
    # Add percentage labels in the middle of each segment
    labels = [f'{v:.1f}%' if v >= 1 else '' for v in c.datavalues]
    ax.bar_label(c, labels=labels, label_type='center', fontsize=10, color='white')

# Customize legend
plt.legend(['Non-Fraud', 'Fraud'], bbox_to_anchor=(1.05, 1), loc='upper left')

# Add grid
plt.grid(axis='x', linestyle='--', alpha=0.3)

# Set background color
ax.set_facecolor('#f8f9fa')
plt.gca().set_axisbelow(True)

# Adjust layout
plt.tight_layout()

# Show plot
plt.show()

# Print total number of transactions and fraud cases per category using category names
category_stats = data.groupby('category_name').agg({
    'fraud': ['count', 'sum']
}).round(2)
category_stats.columns = ['Total Transactions', 'Fraud Cases']
category_stats['Fraud Percentage'] = (category_stats['Fraud Cases'] / category_stats['Total Transactions'] * 100).round(2)
category_stats = category_stats.sort_values('Fraud Percentage', ascending=False)

print("\nDetailed Category Statistics:")
print(category_stats)

# Set figure size
plt.figure(figsize=(18, 7))

# Calculate counts by fraud and merchant
merchant_counts = (data.groupby(['fraud', 'merchant'])
                  .size()
                  .reset_index(name='count')
                  .sort_values('count', ascending=False))

# Filter out specific merchants
merchant_counts = merchant_counts[~merchant_counts['merchant'].isin(['M1823072687', 'M348934600'])]

# Create separate dataframes for fraud and non-fraud
fraud_data = merchant_counts[merchant_counts['fraud'] == 1]
non_fraud_data = merchant_counts[merchant_counts['fraud'] == 0]

# Get unique merchants in order of total transactions
merchant_order = (merchant_counts.groupby('merchant')['count']
                 .sum()
                 .sort_values(ascending=True)
                 .index)

# Create horizontal bar plot
fig, ax = plt.subplots(figsize=(18, 7))

# Plot non-fraud bars
non_fraud_bars = ax.barh(y=range(len(merchant_order)),
                        width=[non_fraud_data[non_fraud_data['merchant'] == m]['count'].values[0]
                              if m in non_fraud_data['merchant'].values else 0
                              for m in merchant_order],
                        color='#97C1A9',  # Light green
                        label='Non-Fraud')

# Plot fraud bars
fraud_bars = ax.barh(y=range(len(merchant_order)),
                     width=[fraud_data[fraud_data['merchant'] == m]['count'].values[0]
                           if m in fraud_data['merchant'].values else 0
                           for m in merchant_order],
                     color='#FF6B6B',  # Light red
                     left=[non_fraud_data[non_fraud_data['merchant'] == m]['count'].values[0]
                           if m in non_fraud_data['merchant'].values else 0
                           for m in merchant_order],
                     label='Fraud')

# Customize plot
plt.title('Fraud for Merchants', pad=20, fontsize=14)

# Set axis labels
plt.xlabel('Frequency')
plt.ylabel('Merchant')

# Set y-axis ticks
plt.yticks(range(len(merchant_order)), merchant_order)

# Remove x-axis ticks
plt.xticks([])

# Add grid
plt.grid(axis='x', linestyle='--', alpha=0.3)

# Add legend
plt.legend()

# Adjust layout
plt.tight_layout()

# Show plot
plt.show()

# Print top merchants by fraud count
print("\nTop Merchants by Fraud Count:")
print(fraud_data.head())

"""# Models Training"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler, LabelEncoder

# processing the data, encode string columns
# Label Encoding age, gender, category
for col in ['gender', 'category']:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])

# Frequency Encoding customer and merchant features
for col in ['customer', 'merchant']:
    freq = data[col].value_counts()
    data[col] = data[col].apply(lambda x: freq.get(x, 0))

X = data.drop(['fraud', 'amount_thresh'], axis=1)
y = data['fraud']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# dealing with imbalanced data
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state = 42)
X_train_sm, y_train_sm = smote.fit_resample(X_train,y_train)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score

# Evaluate the model using precision, recall, f1-score, confusion matrix, ROC curve and auc, Precision-Recall Curve and AUC
def evaluate_model(model, model_name, X_train, y_train, X_test, y_test):

    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Classification report
    print(f"Classification Report for {model_name}:\n")
    print(classification_report(y_test, y_pred))

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix for {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

    # ROC Curve and AUC
    try:
        y_scores = model.predict_proba(X_test)[:, 1]
        fpr, tpr, thresholds_roc = roc_curve(y_test, y_scores)
        roc_auc = auc(fpr, tpr)

        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (area = {roc_auc:.2f})')
        plt.plot([0, 1], [0, 1], color='darkgray', linestyle='--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ROC Curve for {model_name}')
        plt.legend(loc='lower right')
        plt.show()
    except AttributeError:
        print(f"{model_name} does not support predict_proba, ROC curve cannot be plotted.")

    # Precision-Recall Curve and AUC
    try:
        precision, recall, thresholds_pr = precision_recall_curve(y_test, y_scores)
        pr_auc = auc(recall, precision)

        plt.figure(figsize=(8, 6))
        plt.plot(recall, precision, color='green', label=f'PR Curve (area = {pr_auc:.2f})')
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title(f'Precision-Recall Curve for {model_name}')
        plt.legend(loc='best')
        plt.show()
    except AttributeError:
        print(f"{model_name} does not support predict_proba, PR curve cannot be plotted.")

"""## Random Forest

### 1. Use imbalanced data
"""

from sklearn.ensemble import RandomForestClassifier

# load the model
random_forest_model = RandomForestClassifier()

# Evaluate the model
evaluate_model(random_forest_model, "Random Forest Classifier", X_train, y_train, X_test, y_test)

"""### 2. Use resampled data"""

# Random Forest with SMOTE
from sklearn.ensemble import RandomForestClassifier
random_forest_model = RandomForestClassifier()
evaluate_model(random_forest_model, "Random Forest Classifier",
              X_train_sm, y_train_sm,  # Use resampled training data
              X_test, y_test)          # Use original test data

"""## K Neighbors Classifier (KNN)

### 1. Use imbalanced data
"""

# importing the KNN library
from sklearn.neighbors import KNeighborsClassifier

# train and evaluate the KNN model
knn = KNeighborsClassifier(n_neighbors=5)
evaluate_model(knn, "KNN Classifier", X_train, y_train, X_test, y_test)

"""### 2. Use resampled data"""

# KNN with SMOTE
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)
evaluate_model(knn, "KNN Classifier",
              X_train_sm, y_train_sm,  # Use resampled training data
              X_test, y_test)          # Use original test data

"""## Logistic Regression

### 1. Use imbalanced data
"""

from sklearn.linear_model import LogisticRegression

# load the model
logistic_regression_model = LogisticRegression(max_iter=1000, class_weight='balanced')

# Evaluate the model
evaluate_model(logistic_regression_model, "Logistic Regression",  X_train, y_train, X_test, y_test)

"""### 2. Use resampled data"""

# Logistic Regression with SMOTE
from sklearn.linear_model import LogisticRegression
logistic_regression_model = LogisticRegression(max_iter=1000, class_weight='balanced')
evaluate_model(logistic_regression_model, "Logistic Regression",
              X_train_sm, y_train_sm,  # Use resampled training data
              X_test, y_test)          # Use original test data

"""## XGBoost Classifier

### 1. Use imbalanced data
"""

from xgboost import XGBClassifier

# load the model
xgb_classifier_model = XGBClassifier()

# Evaluate the model
evaluate_model(xgb_classifier_model, "XGBoost Classifier", X_train, y_train, X_test, y_test)

"""### 2. Use resampled data"""

# XGBoost with SMOTE
from xgboost import XGBClassifier
xgb_classifier_model = XGBClassifier()
evaluate_model(xgb_classifier_model, "XGBoost Classifier",
              X_train_sm, y_train_sm,  # Use resampled training data
              X_test, y_test)          # Use original test data

"""## Multilayer Perceptron (MLP)

### 1. Use imbalanced data
"""

from sklearn.neural_network import MLPClassifier

# load the model
mlp_model = MLPClassifier(hidden_layer_sizes=(100,),  # hidden layer size
                          max_iter=1000,             # max iter size
                          activation='relu',         # activation function
                          solver='adam',             # optimizer
                          alpha=0.0001,              # L2 regulation
                          random_state=42,
                          early_stopping=True)       # early stopping

# Evaluate the model
evaluate_model(mlp_model, "MLP Classifier", X_train, y_train, X_test, y_test)

"""### 2. Use resampled data"""

# load the model
mlp_model = MLPClassifier(hidden_layer_sizes=(100,),  # hidden layer size
                          max_iter=1000,             # max iter size
                          activation='relu',         # activation function
                          solver='adam',             # optimizer
                          alpha=0.0001,              # L2 regulation
                          random_state=42,
                          early_stopping=True)       # early stopping

# Evaluate the model
evaluate_model(mlp_model, "MLP Classifier", X_train_sm, y_train_sm, X_test, y_test)

"""## Fine-tune the best model using K-Fold cross-validation and train the model
Based on the evaluations of models above, the XGBoost classifier has the best performance. So we choose it as the final model and apply K-Fold cross-validation to find the best hyperpaprameters, then train the model with the whole training dataset and evaluate the performance.
"""

from sklearn.model_selection import KFold, GridSearchCV

# Define the best model based on previous evaluation
best_model = XGBClassifier()  # XGBClassifier performed well

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
}

# Initialize KFold cross-validator
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(
    estimator=best_model,
    param_grid=param_grid,
    scoring='roc_auc',  # Use an appropriate scoring metric
    cv=kfold,
    n_jobs=-1  # Use all available cores
)


# Fit GridSearchCV on the training data with SMOTE
grid_search.fit(X_train_sm, y_train_sm)

# Print the best hyperparameters
print("Best hyperparameters:", grid_search.best_params_)

# Evaluate the best model
best_xgb_model = grid_search.best_estimator_
evaluate_model(best_xgb_model, "Tuned XGBClassifier", X_train, y_train, X_test, y_test)